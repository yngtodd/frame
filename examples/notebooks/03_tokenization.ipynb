{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from datasets import load_dataset\n",
    "from frame.framenet import data_paths\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_examples(dataset, num_examples=5):\n",
    "    \"\"\"Display `num_examples` of the `dataset`\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace DatasetDict from preprocessed framenet\n",
    "        num_examples: number of random examples to display\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for _ in range(num_examples):\n",
    "        example = random.randint(0, len(dataset)-1)\n",
    "        samples.append(example)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[samples])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root path of json data preprocessed by `frame`\n",
    "datapath = \"/Users/ygx/dat/frames/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires having proprocessed the data using `frame`'s\n",
    "# frame.cli:preprocess-framenet\n",
    "paths = data_paths(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-759eabcb88cab5b7\n",
      "Reusing dataset json (/Users/ygx/.cache/huggingface/datasets/json/default-759eabcb88cab5b7/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n"
     ]
    }
   ],
   "source": [
    "# picking the right file format is half the battle\n",
    "dataset = load_dataset('json', data_files=paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['frame', 'sentence', 'frame_definition'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': pyarrow.Table\n",
       " frame: string\n",
       " sentence: string\n",
       " frame_definition: string}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>sentence</th>\n",
       "      <th>frame_definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Familiarity</td>\n",
       "      <td>It was also the time when the region was the h...</td>\n",
       "      <td>An Entity is presented as having been seen or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Attack</td>\n",
       "      <td>The goal of the current United States-led offe...</td>\n",
       "      <td>An Assailant physically attacks a Victim (whic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feigning</td>\n",
       "      <td>She feigned enthusiasm , but what she was real...</td>\n",
       "      <td>An Agent acts in such a way as to give the inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Body_parts</td>\n",
       "      <td>Only other white-breasted duck is short-necked...</td>\n",
       "      <td>This frame covers words for Body_part(s) (BP) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Body_description_holistic</td>\n",
       "      <td>Stocky and friendly , he had two small flaws i...</td>\n",
       "      <td>This frame covers descriptions of an entire hu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_examples(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Seq2Seq Data with T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [20439, 2925, 12, 2662, 115, 1152, 550, 3, 6, 112, 1234, 1413, 4339, 3843, 28, 385, 91, 5808, 7, 17, 7, 13, 22496, 3, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokenizer itself returns input_ids and base attention masks\n",
    "tokenizer(dataset[\"train\"]['sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize the data for Seq2Seq\n",
    "    \n",
    "    Maps over all the examples in the dataset \n",
    "    to tokenize both the input framenet sentences\n",
    "    and the target frame definitions.\n",
    "    \n",
    "    Args:\n",
    "        examples: samples in the dataset\n",
    "    \"\"\"\n",
    "    inputs = [sent for sent in examples[\"sentence\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_input_length, truncation=True\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"frame_definition\"], \n",
    "            max_length=max_target_length, \n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[377, 1], [3, 52, 1], [3, 9, 1], [3, 29, 1], [3, 75, 1], [3, 32, 1], [1], [3, 75, 1], [3, 32, 1], [3, 29, 1], [3, 17, 1], [3, 23, 1], [3, 29, 1], [3, 76, 1], [3, 15, 1], [3, 26, 1], [1], [3, 17, 1], [3, 32, 1], [1], [3, 354, 1], [3, 9, 1], [3, 115, 1], [3, 115, 1], [3, 15, 1], [3, 52, 1], [1], [3, 9, 1], [3, 210, 1], [3, 9, 1], [3, 63, 1], [1], [3, 6, 1], [1], [3, 107, 1], [3, 23, 1], [3, 7, 1], [1], [3, 210, 1], [3, 32, 1], [3, 52, 1], [3, 26, 1], [3, 7, 1], [1], [3, 23, 1], [3, 29, 1], [3, 17, 1], [3, 15, 1], [3, 52, 1], [3, 7, 1], [3, 102, 1], [3, 15, 1], [3, 52, 1], [3, 7, 1], [3, 15, 1], [3, 26, 1], [1], [3, 210, 1], [3, 23, 1], [3, 17, 1], [3, 107, 1], [1], [3, 40, 1], [3, 23, 1], [3, 17, 1], [3, 17, 1], [3, 40, 1], [3, 15, 1], [1], [3, 32, 1], [3, 76, 1], [3, 17, 1], [3, 115, 1], [3, 76, 1], [3, 52, 1], [3, 7, 1], [3, 17, 1], [3, 7, 1], [1], [3, 32, 1], [3, 89, 1], [1], [3, 40, 1], [3, 9, 1], [3, 76, 1], [3, 122, 1], [3, 107, 1], [3, 17, 1], [3, 15, 1], [3, 52, 1], [1], [3, 5, 1], [1]], 'attention_mask': [[1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1], [1, 1, 1], [1, 1, 1], [1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1], [1, 1, 1], [1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1], [1, 1, 1], [1, 1, 1], [1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1], [1, 1, 1], [1]], 'labels': [37, 1234, 16, 48, 2835, 5530, 6362, 277, 13, 7375, 138, 1901, 5, 432, 13, 135, 54, 4093, 28, 16854, 3893, 7, 5, 3, 31, 3845, 3, 7, 40, 450, 1271, 112, 26838, 5, 31, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek into how preprocessing looks for the first example\n",
    "preprocess_function(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcaa92354a4d4fd9887ee4788b2a09d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'frame', 'frame_definition', 'input_ids', 'labels', 'sentence'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
